{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_jay_lyrics():\n",
    "    \"\"\"Load the Jay Chou lyric data set (available in the Chinese book).\"\"\"\n",
    "    with zipfile.ZipFile('../../data/jaychou_lyrics.txt.zip') as zin:\n",
    "        with zin.open('jaychou_lyrics.txt') as f:\n",
    "            corpus_chars = f.read().decode('utf-8')\n",
    "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "    vocab_size = len(char_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_indices, char_to_idx, idx_to_char, vocab_size = load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "\n",
    "现在我们考虑输入数据存在时间相关性的情况。假设$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$是序列中时间步$t$的小批量输入,$\\boldsymbol{H}_t \\in \\mathbb{R}^{n \\times d}$是该时间步的隐藏变量。与多层感知机不同的是，这里我们保存上一时间步的隐藏变量$\\boldsymbol{H}_{t-1}$，并引入一个新的权重参数$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步$t$的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：\n",
    "$$\\boldsymbol{H}_t = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}  + \\boldsymbol{b}_h).$$\n",
    "\n",
    "与多层感知机相比，我们在这里添加了$\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh} $ 一项。由上式中相邻时间步的隐藏变量$\\boldsymbol{X}_t$和$\\boldsymbol{X}_{t-1}$之间的关系可知，这里的隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络（recurrent neural network）。\n",
    "\n",
    "循环神经网络有很多种不同的构造方法。含上式所定义的隐藏状态的循环神经网络是极为常见的一种。若无特别说明，本章中的循环神经网络均基于上式中隐藏状态的循环计算。在时间步$t$，输出层的输出和多层感知机中的计算类似：\n",
    "$$\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q.$$\n",
    "\n",
    "循环神经网络的参数包括隐藏层的权重 $\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$、 $\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$ 和偏差  $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$ ，以及输出层的权重 $\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$ 和偏差 $\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。值得一提的是，即便在不同时间步，循环神经网络也始终使用这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,w_xh = tf.random.normal(shape=(3,2)),tf.random.normal(shape=(2,4))\n",
    "h,w_hh = tf.random.normal(shape=(3,4)),tf.random.normal(shape=(4,4))\n",
    "b_h = tf.random.normal(shape=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=32, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 0.9330221 , -1.7209074 , -2.7449512 ,  2.4443598 ],\n",
       "       [ 2.6006992 ,  1.5038726 , -0.73471355,  0.32491297],\n",
       "       [-1.0247426 , -1.8536962 , -2.6441722 ,  2.626462  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_t = tf.matmul(x,w_xh)+tf.matmul(h,w_hh)\n",
    "h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=37, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 0.9330221, -1.7209076, -2.7449512,  2.4443598],\n",
       "       [ 2.6006992,  1.5038726, -0.7347137,  0.3249129],\n",
       "       [-1.0247426, -1.8536962, -2.6441724,  2.626462 ]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(tf.concat([x,h],axis=1),tf.concat([w_xh,w_hh],axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=42, shape=(2, 1027), dtype=float32, numpy=\n",
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.one_hot([0,2],depth=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(X,depth):\n",
    "    outs = [tf.one_hot(x,depth=depth) for x in tf.transpose(X)]\n",
    "    return tf.stack(outs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=86, shape=(5, 2, 1027), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(range(10)).reshape((2,5))\n",
    "to_onehot(x,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs,params,state):\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H=state\n",
    "    outputs = []\n",
    "    for x in inputs:\n",
    "        a = tf.matmul(x,W_xh)\n",
    "        b = tf.matmul(H,W_hh)\n",
    "        H = tf.tanh( a + b + b_h)\n",
    "        outputs.append( tf.keras.activations.softmax(tf.matmul(H,W_hq) + b_q))\n",
    "        \n",
    "    return tf.stack(outputs),H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "\n",
    "    W_xh = tf.random.normal(shape=(num_inputs,num_hiddens))\n",
    "    W_hh = tf.random.normal(shape=(num_hiddens,num_hiddens))\n",
    "    b_h = tf.zeros(shape=(1,num_hiddens))\n",
    "\n",
    "    W_hq = tf.random.normal(shape=(num_hiddens,num_outputs))\n",
    "    b_q = tf.zeros(shape=(1,num_outputs))\n",
    "    \n",
    "    return W_xh,W_hh,b_h,W_hq,b_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens):\n",
    "    return tf.zeros(shape=(batch_size, num_hiddens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_rnn_state(x.shape[0],num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=218, shape=(5, 2, 1027), dtype=float32, numpy=\n",
       " array([[[1.0269333e-15, 5.2755109e-13, 1.0907504e-12, ...,\n",
       "          2.5514712e-19, 3.9393851e-16, 4.4626074e-19],\n",
       "         [1.2050021e-19, 3.8631719e-14, 1.4777405e-19, ...,\n",
       "          2.8166343e-09, 7.3760730e-15, 4.5071572e-15]],\n",
       " \n",
       "        [[4.6540255e-21, 1.6330873e-21, 8.4880560e-32, ...,\n",
       "          7.7777580e-19, 1.5149020e-26, 9.5517963e-31],\n",
       "         [3.2947158e-23, 1.4410954e-13, 4.7374664e-22, ...,\n",
       "          3.4390364e-27, 2.4437146e-29, 1.2373804e-21]],\n",
       " \n",
       "        [[6.9397681e-17, 8.7288145e-27, 4.3863918e-21, ...,\n",
       "          1.2644458e-20, 1.0586329e-15, 5.1568602e-18],\n",
       "         [1.6680184e-24, 4.1121730e-20, 2.4531889e-19, ...,\n",
       "          2.9208754e-06, 2.0136901e-11, 8.0009505e-10]],\n",
       " \n",
       "        [[6.2488128e-31, 1.6814829e-12, 5.6531156e-22, ...,\n",
       "          1.8573003e-22, 3.1143443e-12, 1.9550581e-15],\n",
       "         [3.7855802e-17, 4.4262974e-06, 2.5365010e-17, ...,\n",
       "          4.3525763e-31, 3.7682961e-14, 3.9637309e-16]],\n",
       " \n",
       "        [[1.3057465e-12, 1.3747934e-20, 1.7444925e-18, ...,\n",
       "          4.9073607e-23, 8.6177207e-14, 1.7482890e-19],\n",
       "         [9.5947872e-18, 2.4218375e-25, 9.4092431e-33, ...,\n",
       "          6.4350324e-17, 7.5691620e-27, 2.2152564e-20]]], dtype=float32)>,\n",
       " <tf.Tensor: id=214, shape=(2, 256), dtype=float32, numpy=\n",
       " array([[ 1.        , -0.9900137 ,  1.        ,  0.9999981 , -0.9999579 ,\n",
       "         -1.        , -0.8679179 , -0.8547947 , -0.99650073, -1.        ,\n",
       "          1.        ,  1.        , -1.        , -1.        , -1.        ,\n",
       "         -1.        ,  0.9999994 ,  0.9943114 ,  1.        ,  0.99929714,\n",
       "          0.99999905, -1.        ,  1.        , -0.9999997 , -1.        ,\n",
       "         -1.        ,  1.        ,  0.9837422 ,  0.99570614, -1.        ,\n",
       "         -1.        ,  1.        ,  0.986669  ,  0.9965036 ,  1.        ,\n",
       "          1.        ,  0.99068207, -1.        , -1.        ,  0.65099555,\n",
       "          1.        , -1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         -0.8747794 , -1.        , -0.9999999 ,  0.9999993 ,  0.74847317,\n",
       "         -1.        , -1.        ,  0.999978  ,  0.9999973 , -0.9679399 ,\n",
       "          0.6903402 ,  1.        ,  0.9999999 , -1.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        ,  0.99999976, -0.99923587,\n",
       "          0.9999998 , -0.34507838, -1.        , -0.5454634 ,  0.57487166,\n",
       "         -1.        ,  1.        ,  1.        ,  0.9999945 , -1.        ,\n",
       "         -1.        , -0.01506948,  0.9999999 , -0.8455941 , -0.9999999 ,\n",
       "         -0.99999934, -1.        ,  1.        ,  1.        , -1.        ,\n",
       "          0.9999999 ,  0.99977195,  1.        ,  1.        ,  1.        ,\n",
       "         -1.        ,  0.9994166 ,  1.        ,  1.        , -1.        ,\n",
       "          1.        ,  0.99987614, -1.        ,  1.        ,  1.        ,\n",
       "          0.72822756,  1.        ,  1.        ,  0.9353107 , -0.99999875,\n",
       "          0.29819435,  1.        , -0.9999995 ,  1.        ,  1.        ,\n",
       "          1.        ,  0.9999613 ,  0.99890107,  0.99871993, -1.        ,\n",
       "         -1.        , -0.16975833, -0.7918883 ,  1.        , -1.        ,\n",
       "          1.        , -1.        ,  0.9999481 ,  1.        ,  0.9996997 ,\n",
       "         -1.        , -0.9959007 , -0.99998444,  0.999987  , -1.        ,\n",
       "         -1.        , -1.        , -0.99932873, -1.        , -1.        ,\n",
       "          1.        ,  0.9999981 , -0.9957415 , -1.        , -1.        ,\n",
       "          0.99999726,  1.        , -1.        ,  0.9680368 ,  1.        ,\n",
       "          1.        , -0.9999954 , -1.        ,  1.        ,  0.93930006,\n",
       "         -1.        , -1.        ,  0.99408966,  1.        ,  0.9999608 ,\n",
       "         -1.        , -0.9986219 , -1.        ,  0.9999999 , -0.6535528 ,\n",
       "          1.        ,  0.99999994,  0.99989605,  0.99703664,  1.        ,\n",
       "          1.        , -0.9996014 , -0.996537  , -0.99779266,  0.9995759 ,\n",
       "          0.99954146,  0.9999975 ,  1.        ,  0.99160707,  0.9999992 ,\n",
       "         -0.9999986 ,  1.        ,  0.70526016, -0.98692423, -1.        ,\n",
       "         -0.61726964, -0.9999804 , -0.9907564 ,  1.        , -0.9999999 ,\n",
       "         -1.        ,  1.        ,  0.99932396,  1.        , -0.5730584 ,\n",
       "          1.        , -0.98925406,  0.99983984,  0.9709694 , -1.        ,\n",
       "          1.        ,  0.9999258 ,  1.        , -1.        , -1.        ,\n",
       "         -1.        ,  0.9976111 ,  1.        , -1.        , -1.        ,\n",
       "         -1.        , -1.        , -0.99998516,  0.99999976, -0.6147871 ,\n",
       "          1.        , -0.99999964,  1.        ,  0.9997374 , -0.99965805,\n",
       "          0.99647266, -0.99999976,  0.95780814,  1.        , -1.        ,\n",
       "          0.99998647,  0.99999994, -1.        , -1.        ,  0.9930054 ,\n",
       "         -0.99999976, -0.90931404, -1.        ,  1.        ,  0.9999306 ,\n",
       "         -1.        , -1.        ,  1.        ,  0.9999977 , -1.        ,\n",
       "         -0.9973912 ,  1.        ,  1.        ,  1.        ,  0.9999998 ,\n",
       "          0.99996006, -1.        ,  1.        ,  0.99960154, -1.        ,\n",
       "         -0.9913136 ,  1.        , -1.        , -1.0000001 ,  0.71468824,\n",
       "         -0.66429275, -0.37517494,  1.        ,  1.        ,  0.99948597,\n",
       "         -0.9999998 ],\n",
       "        [ 1.        , -1.        ,  1.        ,  0.99737984,  1.        ,\n",
       "         -0.999996  ,  1.        ,  0.9847717 , -1.        ,  1.        ,\n",
       "          0.91752166, -0.9999907 , -1.        ,  1.        , -0.93943346,\n",
       "          1.        , -1.        ,  1.        , -0.9999937 , -0.99999356,\n",
       "          1.        ,  1.        , -1.        ,  1.        ,  1.        ,\n",
       "         -1.        , -1.        , -0.9999999 ,  1.        ,  0.99999964,\n",
       "         -1.        , -1.        ,  0.9999999 , -1.        , -1.        ,\n",
       "         -0.8698212 , -1.        ,  1.        , -1.        ,  1.        ,\n",
       "         -1.        ,  1.        ,  1.        ,  1.        , -1.        ,\n",
       "          1.        , -1.        , -0.9999924 ,  1.        ,  1.        ,\n",
       "         -1.        , -1.        , -1.        ,  1.        ,  0.52952456,\n",
       "          0.76633286,  0.9999999 ,  0.86400425, -1.        ,  0.99998546,\n",
       "         -0.7635282 ,  0.7392794 , -0.99999875,  0.9974666 ,  1.        ,\n",
       "         -0.9412348 ,  0.9999944 , -0.98694986,  1.        , -1.        ,\n",
       "         -1.        ,  1.        ,  0.84657586, -0.9999997 ,  0.99995005,\n",
       "          1.        , -0.88525933,  0.66308224, -1.        , -1.        ,\n",
       "         -1.        ,  1.        , -0.9999996 ,  0.98517233, -1.        ,\n",
       "         -1.        ,  0.9999755 , -0.9999937 , -1.        ,  1.        ,\n",
       "         -1.        ,  0.9999022 ,  0.9973987 , -1.        ,  0.99044865,\n",
       "         -1.        , -0.85164064, -0.9999991 ,  1.        ,  0.9994053 ,\n",
       "         -0.9999999 , -0.998891  ,  1.        ,  1.        ,  1.        ,\n",
       "          1.        , -0.9992757 ,  0.96970177,  1.        , -1.        ,\n",
       "          1.        , -1.        , -1.        ,  1.        ,  0.99737936,\n",
       "          0.9972647 ,  0.9838299 ,  0.9992007 , -1.        ,  0.32303667,\n",
       "          1.        , -1.        ,  0.9859437 , -1.        , -0.9931602 ,\n",
       "         -1.        ,  1.        ,  1.        , -0.99999195,  0.99348074,\n",
       "         -0.9999924 ,  1.        , -0.99999994,  1.        , -1.        ,\n",
       "         -0.9942412 , -0.9999997 ,  1.        , -1.        , -0.9986905 ,\n",
       "         -1.        , -1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         -0.89508075, -1.        ,  1.        ,  1.        ,  0.99994284,\n",
       "          0.99991536,  0.76254743, -1.        , -0.77429235,  0.99998367,\n",
       "         -0.9999998 , -0.9995166 , -1.        ,  1.        , -1.        ,\n",
       "         -1.        ,  0.9975196 ,  1.        ,  1.        ,  1.        ,\n",
       "         -0.99999577,  1.        , -1.        ,  0.99999803,  0.9997894 ,\n",
       "          0.99859107,  1.        , -1.        ,  1.        , -1.        ,\n",
       "         -0.9998814 , -0.3841898 ,  1.        , -1.        , -0.9999184 ,\n",
       "         -1.        ,  1.        , -1.        ,  1.        ,  1.        ,\n",
       "          0.97276413, -1.        ,  0.99904716,  1.        , -0.9766493 ,\n",
       "          0.9999912 , -0.30296588,  1.        , -0.844461  ,  1.        ,\n",
       "          0.9999999 , -1.        , -0.93261373, -1.        ,  0.9837692 ,\n",
       "         -1.        ,  1.        , -0.9965279 ,  1.        , -1.        ,\n",
       "          0.7723631 ,  0.9938513 , -0.99238443, -0.99974215,  0.9999978 ,\n",
       "         -0.7790277 ,  1.        , -0.88058776, -1.        , -1.        ,\n",
       "          0.9960917 , -1.        ,  1.        ,  1.        , -1.        ,\n",
       "          0.9999263 , -0.9987085 , -1.        ,  1.        ,  0.93188006,\n",
       "          1.        ,  0.99999964, -0.99979967,  1.        ,  1.        ,\n",
       "         -1.        , -1.        ,  0.9999998 , -1.        ,  1.        ,\n",
       "         -0.9999999 , -0.9999998 ,  1.        , -0.9998685 ,  0.9999986 ,\n",
       "          0.9840061 , -0.98918754, -0.9999996 ,  0.99395883,  1.        ,\n",
       "          0.99999946, -0.993745  ,  0.9999962 , -0.9995383 ,  1.        ,\n",
       "         -0.9998998 ,  0.39455724, -0.9999982 ,  1.        , -0.92452   ,\n",
       "         -1.        ]], dtype=float32)>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = to_onehot(x,vocab_size)\n",
    "rnn(inputs,init_params(),state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size, idx_to_char, char_to_idx):\n",
    "    state = init_rnn_state(1, num_hiddens)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = to_onehot([output[-1]], vocab_size)\n",
    "        X = tf.expand_dims(X,axis=0)\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, params,state )\n",
    "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(tf.argmax(Y[0],axis=1).numpy()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开痛故肩打抢骷已育肩钩'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('分开', 10, rnn, init_params(), init_rnn_state, num_hiddens, vocab_size, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('../../data/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "corpus_chars = corpus_chars.replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "tokens = jieba.cut(corpus_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.687 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "seg_list = []\n",
    "for i in tokens:\n",
    "    seg_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成onehot\n",
    "vocab = sorted(list(set(seg_list)))\n",
    "word_to_int = dict((w,i) for i,w in enumerate(vocab))\n",
    "int_to_word = dict((i,w) for i,w in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5727, '我', 2475)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab),int_to_word[2475],word_to_int[\"我\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37434, 5727)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = len(seg_list)\n",
    "vocab_size = len(vocab)\n",
    "n_words,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lenght = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0,n_words-seq_lenght,1):\n",
    "    seq_in = seg_list[i:i+seq_lenght+1]\n",
    "    dataX.append([word_to_int[word] for word in seq_in])\n",
    "\n",
    "np.random.shuffle(dataX)\n",
    "for i in range(len(dataX)):\n",
    "    dataY.append([dataX[i][seq_lenght]])\n",
    "    dataX[i] = dataX[i][:seq_lenght]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=607, shape=(100, 5727), dtype=float32, numpy=\n",
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.one_hot(dataX[0],vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数量： 37334\n"
     ]
    }
   ],
   "source": [
    "n_simples = len(dataX)\n",
    "print('样本数量：', n_simples)\n",
    "X = np.reshape(dataX, (n_simples, seq_lenght))\n",
    "y = tf.keras.utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'岁月我用无悔 刻永世爱你的碑啦儿啦 啦儿啦 啦儿啦儿啦啦儿啦 啦儿啦 啦儿啦儿啦铜镜映无邪 紮马尾你若撒野 今生我把酒奉陪啦儿啦 啦儿啦 啦儿啦儿啦啦儿啦 啦儿啦 啦儿啦儿啦铜镜映无邪 紮马尾你若撒野 今生我把酒奉陪一件黑色毛衣两个人的回忆雨过之后更难'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = \"\"\n",
    "for ind in X[0]:\n",
    "    temp += int_to_word[ind] \n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Embedding(vocab_size,16,input_length=seq_lenght),\n",
    "    tf.keras.layers.SimpleRNN(128),\n",
    "    tf.keras.layers.Dense(vocab_size,activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37334 samples\n",
      "Epoch 1/5\n",
      "37334/37334 [==============================] - 22s 592us/sample - loss: 7.0411\n",
      "Epoch 2/5\n",
      "37334/37334 [==============================] - 20s 541us/sample - loss: 6.3995\n",
      "Epoch 3/5\n",
      "37334/37334 [==============================] - 20s 543us/sample - loss: 5.7953\n",
      "Epoch 4/5\n",
      "37334/37334 [==============================] - 20s 541us/sample - loss: 5.2082\n",
      "Epoch 5/5\n",
      "37334/37334 [==============================] - 20s 542us/sample - loss: 4.6887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2af1899b0b8>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "949"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_int[\"分开\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed : \n",
      "霓虹 不安跳动 染红夜空过去种种 象一场梦 不敢去碰 一想就痛心碎内容 每一秒钟 都有不同 你不懂连一句珍重 也有苦衷 也不想送寒风中 废墟烟囱 停止转动 一切落空在人海中 盲目跟从 别人的梦 全面放纵恨没有用 疗伤止痛 不再感动 没有梦痛不知轻重 泪水鲜红 全面放纵恨自己真的没用 情绪激动一颗心到现在还在抽痛\n",
      "开始生成，生成长度为 200\n",
      " 在一种味道叫做家没法挑剔 的窝 我的风火轮 的等候 我中的愿望     你的爱 我会  我的兄弟 放开 使用双截棍 哼哼哈兮快使用双截棍 哼哼哈兮快使用双截棍 哼哼哈兮习武之人切记 仁者无敌   很解药     我的兄弟 放开了窗玻璃     你在爱情 而   我不说 我           喔 喔 喔 哎哟 喔 哎哟 喔 哎哟 喔 哎哟 喔 哎哟 喔 哎哟 喔 哎哟 喔 哎哟 喔 哎哟 喔 哎哟 喔 哎哟 喔 哎哟 喔 哎哟喔喔 啦儿啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦啦"
     ]
    }
   ],
   "source": [
    "# 生成种子\n",
    "start = np.random.randint(0, len(dataX) - 1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed : \")\n",
    "print(''.join([int_to_word[value] for value in pattern]))\n",
    "n_generation = 200  # 生成的长度\n",
    "print('开始生成，生成长度为', n_generation)\n",
    "finall_result = []\n",
    "for i in range(n_generation):\n",
    "    x = np.reshape(pattern, (1, len(pattern)))\n",
    "    prediction = model.predict(x, verbose=0)[0]\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_word[index]\n",
    "    # sys.stdout.write(result)\n",
    "    finall_result.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "# print(finall_result)\n",
    "for i in range(len(finall_result)):\n",
    "    if finall_result[i] != '。':\n",
    "        print(finall_result[i], end='')\n",
    "    else:\n",
    "        print('。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.0",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
